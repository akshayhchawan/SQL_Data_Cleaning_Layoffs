# SQL_Data_Cleaning_Layoffs_Dataset

**Situation:**
The project involved working with a messy layoffs dataset, which contained errors like duplicates, inconsistencies, missing values, and irrelevant columns. This made the data difficult to analyze and unreliable for any insights.

**Task:**
The goal was to clean the dataset, ensuring it was structured, consistent, and ready for analysis. The challenge was to handle a variety of data issues, such as duplicate entries, null values, and standardized formats, to make the dataset accurate and usable.

**Action:**
I applied four key data cleaning techniques:
1.	Removed duplicates - Identified and deleted duplicate entries that could skew analysis.
2.	Standardized data - Ensured consistent naming conventions and formats across the dataset, removing extra spaces or unnecessary characters..
3.	Handled missing values - Populated blank values where possible and removed rows that couldn’t be corrected.
4.	Removed unnecessary columns - Eliminated irrelevant columns that didn’t contribute to the analysis.

**Result:**
The cleaned dataset was structured, consistent, and ready for analysis, ensuring accurate insights could be drawn. It improved the integrity of the data and made it easier to use for future reporting and analysis.

**Key Takeaway:**
Data cleaning is often overlooked, but it’s the backbone of any robust analysis. My work on this project helped me appreciate how meticulous cleaning opens the doors to meaningful insights. Looking forward to applying these techniques to new challenges!
